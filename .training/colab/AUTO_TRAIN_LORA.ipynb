{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ðŸš€ Auto-Train LoRA Adapter for Coding Agent\n",
        "\n",
        "**One-click training on free GPU!**\n",
        "\n",
        "This notebook automatically:\n",
        "1. âœ… Downloads your SFT data from GitHub\n",
        "2. âœ… Trains LoRA adapter on free T4 GPU\n",
        "3. âœ… Converts to GGUF for Ollama\n",
        "4. âœ… Uploads back to GitHub\n",
        "\n",
        "**Just click Runtime â†’ Run All!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## âš™ï¸ Configuration\n",
        "\n",
        "Edit these settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_cell"
      },
      "outputs": [],
      "source": [
        "# GitHub repository (format: username/repo)\n",
        "GITHUB_REPO = \"christcr2012/robinsonai-mcp-servers\"\n",
        "\n",
        "# Branch name\n",
        "GITHUB_BRANCH = \"feat/repo-guardrails\"\n",
        "\n",
        "# Role to train (coder, fixer, or judge)\n",
        "ROLE = \"coder\"\n",
        "\n",
        "# Base model\n",
        "BASE_MODEL = \"unsloth/qwen2.5-coder-7b-bnb-4bit\"\n",
        "\n",
        "# Training parameters\n",
        "LORA_RANK = 16\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_STEPS = 100\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# GitHub token (optional - for auto-upload)\n",
        "# Get from: https://github.com/settings/tokens\n",
        "GITHUB_TOKEN = \"\"  # Leave empty for manual download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## ðŸ“¦ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q torch transformers datasets peft accelerate bitsandbytes trl\n",
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## ðŸ“¥ Download SFT Data from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_cell"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Download SFT data\n",
        "sft_url = f\"https://raw.githubusercontent.com/{GITHUB_REPO}/{GITHUB_BRANCH}/.agent/sft/{ROLE}_sft.jsonl\"\n",
        "\n",
        "print(f\"ðŸ“¥ Downloading SFT data from: {sft_url}\")\n",
        "response = requests.get(sft_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open('sft_data.jsonl', 'w') as f:\n",
        "        f.write(response.text)\n",
        "    \n",
        "    # Count examples\n",
        "    with open('sft_data.jsonl') as f:\n",
        "        examples = [json.loads(line) for line in f if line.strip()]\n",
        "    \n",
        "    print(f\"âœ… Downloaded {len(examples)} training examples\")\n",
        "    print(f\"\\nðŸ“ Sample example:\")\n",
        "    print(json.dumps(examples[0], indent=2))\n",
        "else:\n",
        "    print(f\"âŒ Failed to download: {response.status_code}\")\n",
        "    print(f\"   Make sure the file exists at: .agent/sft/{ROLE}_sft.jsonl\")\n",
        "    raise Exception(\"Download failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## ðŸ“Š Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data_cell"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('json', data_files='sft_data.jsonl', split='train')\n",
        "\n",
        "print(f\"âœ… Dataset loaded: {len(dataset)} examples\")\n",
        "print(f\"\\nðŸ“‹ Dataset structure:\")\n",
        "print(dataset)\n",
        "print(f\"\\nðŸ“ First example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## ðŸ¤– Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model_cell"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(f\"ðŸ”¥ Loading model: {BASE_MODEL}\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=BASE_MODEL,\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"âœ… Model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_lora"
      },
      "source": [
        "## ðŸ”§ Add LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add_lora_cell"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_RANK,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    lora_alpha=LORA_RANK,\n",
        "    lora_dropout=0,\n",
        "    bias='none',\n",
        "    use_gradient_checkpointing='unsloth',\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(f\"âœ… LoRA adapters added (rank={LORA_RANK})\")\n",
        "print(f\"\\nðŸ“Š Trainable parameters:\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "format"
      },
      "source": [
        "## ðŸ“ Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "format_cell"
      },
      "outputs": [],
      "source": [
        "def format_prompts(examples):\n",
        "    texts = []\n",
        "    for prompt, completion in zip(examples['prompt'], examples['completion']):\n",
        "        # Format as instruction-response pair\n",
        "        text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{completion}\"\n",
        "        texts.append(text)\n",
        "    return {'text': texts}\n",
        "\n",
        "dataset = dataset.map(format_prompts, batched=True)\n",
        "\n",
        "print(\"âœ… Dataset formatted\")\n",
        "print(f\"\\nðŸ“ Formatted example:\")\n",
        "print(dataset[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## ðŸš€ Train LoRA Adapter\n",
        "\n",
        "This will take ~10-15 minutes on free T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_cell"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field='text',\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        max_steps=MAX_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim='adamw_8bit',\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type='linear',\n",
        "        seed=3407,\n",
        "        output_dir='outputs',\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "print(f\"   Steps: {MAX_STEPS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "print()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## ðŸ’¾ Save Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_cell"
      },
      "outputs": [],
      "source": [
        "# Save LoRA adapter\n",
        "model.save_pretrained('lora_adapter')\n",
        "tokenizer.save_pretrained('lora_adapter')\n",
        "\n",
        "print(\"âœ… LoRA adapter saved to: lora_adapter/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert"
      },
      "source": [
        "## ðŸ”„ Convert to GGUF for Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_cell"
      },
      "outputs": [],
      "source": [
        "# Convert to GGUF format\n",
        "model.save_pretrained_gguf(\n",
        "    'lora_adapter_gguf',\n",
        "    tokenizer,\n",
        "    quantization_method='q4_k_m'\n",
        ")\n",
        "\n",
        "print(\"âœ… Converted to GGUF format\")\n",
        "print(\"   Location: lora_adapter_gguf/\")\n",
        "\n",
        "# List files\n",
        "!ls -lh lora_adapter_gguf/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_adapter"
      },
      "source": [
        "## ðŸ“¥ Download Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_adapter_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create zip file\n",
        "!zip -r lora_adapter.zip lora_adapter_gguf/\n",
        "\n",
        "print(\"ðŸ“¦ Created lora_adapter.zip\")\n",
        "print(f\"   Size: {os.path.getsize('lora_adapter.zip') / 1024 / 1024:.2f} MB\")\n",
        "print()\n",
        "print(\"â¬‡ï¸  Downloading...\")\n",
        "\n",
        "files.download('lora_adapter.zip')\n",
        "\n",
        "print(\"\\nâœ… Download complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deploy"
      },
      "source": [
        "## ðŸš€ Deploy to Ollama (Instructions)\n",
        "\n",
        "After downloading `lora_adapter.zip`, follow these steps on your local machine:\n",
        "\n",
        "### 1. Extract the adapter\n",
        "```bash\n",
        "# Extract to .agent/lora/coder/\n",
        "unzip lora_adapter.zip -d .agent/lora/coder/\n",
        "```\n",
        "\n",
        "### 2. Create Modelfile\n",
        "```bash\n",
        "cat > .agent/Modelfile.coder <<EOF\n",
        "FROM qwen2.5-coder:7b\n",
        "ADAPTER .agent/lora/coder/lora_adapter_gguf/adapter.gguf\n",
        "PARAMETER temperature 0.2\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER top_k 40\n",
        "EOF\n",
        "```\n",
        "\n",
        "### 3. Deploy to Ollama\n",
        "```bash\n",
        "ollama create my-coder-tuned-coder -f .agent/Modelfile.coder\n",
        "```\n",
        "\n",
        "### 4. Test it!\n",
        "```bash\n",
        "ollama run my-coder-tuned-coder\n",
        "```\n",
        "\n",
        "### 5. Update model variants\n",
        "The learning system will automatically detect and use the new model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auto_upload"
      },
      "source": [
        "## ðŸ”„ Auto-Upload to GitHub (Optional)\n",
        "\n",
        "If you provided a GitHub token, this will automatically upload the adapter back to your repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auto_upload_cell"
      },
      "outputs": [],
      "source": [
        "if GITHUB_TOKEN:\n",
        "    print(\"ðŸ”„ Uploading to GitHub...\")\n",
        "    \n",
        "    # Clone repo\n",
        "    !git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git repo\n",
        "    !cd repo && git checkout {GITHUB_BRANCH}\n",
        "    \n",
        "    # Copy adapter\n",
        "    !mkdir -p repo/.agent/lora/{ROLE}\n",
        "    !cp -r lora_adapter_gguf/* repo/.agent/lora/{ROLE}/\n",
        "    \n",
        "    # Create Modelfile\n",
        "    modelfile = f\"\"\"FROM qwen2.5-coder:7b\n",
        "ADAPTER .agent/lora/{ROLE}/adapter.gguf\n",
        "PARAMETER temperature 0.2\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER top_k 40\n",
        "\"\"\"\n",
        "    with open(f'repo/.agent/Modelfile.{ROLE}', 'w') as f:\n",
        "        f.write(modelfile)\n",
        "    \n",
        "    # Commit and push\n",
        "    !cd repo && git config user.email \"colab@auto-train.com\"\n",
        "    !cd repo && git config user.name \"Colab Auto-Train\"\n",
        "    !cd repo && git add .agent/lora/{ROLE}/ .agent/Modelfile.{ROLE}\n",
        "    !cd repo && git commit -m \"chore: Auto-trained LoRA adapter for {ROLE}\"\n",
        "    !cd repo && git push\n",
        "    \n",
        "    print(\"âœ… Uploaded to GitHub!\")\n",
        "else:\n",
        "    print(\"â­ï¸  Skipping auto-upload (no GitHub token provided)\")\n",
        "    print(\"   Download lora_adapter.zip manually and deploy locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "done"
      },
      "source": [
        "## ðŸŽ‰ Done!\n",
        "\n",
        "Your LoRA adapter has been trained and is ready to deploy!\n",
        "\n",
        "**Next steps:**\n",
        "1. âœ… Download `lora_adapter.zip` (already done)\n",
        "2. âœ… Extract to `.agent/lora/coder/`\n",
        "3. âœ… Deploy to Ollama (see instructions above)\n",
        "4. âœ… Test your custom model!\n",
        "\n",
        "**Expected improvements:**\n",
        "- +10-20% compile rate\n",
        "- +20-30% convention score\n",
        "- Model knows your codebase patterns\n",
        "- Fewer iterations per task\n",
        "\n",
        "**To train again:**\n",
        "- Just run this notebook again when you have more data!\n",
        "- The model will continue to improve with more examples"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

